Output:  /zhome/77/9/213690/biometrics/out/VIT-21k/Run_0-6
Epochs:  2
Learning rate:  0.0001
Dataset:  right_eyes_only
Batch size:  16
Labels of Dataset: ['female', 'male']
Loaded accuracy
ViTImageProcessor {
  "do_convert_rgb": null,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "ViTImageProcessor",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "height": 224,
    "width": 224
  }
}

ViTForImageClassification(
  (vit): ViTModel(
    (embeddings): ViTEmbeddings(
      (patch_embeddings): ViTPatchEmbeddings(
        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): ViTEncoder(
      (layer): ModuleList(
        (0-11): 12 x ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  )
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
{'loss': 0.6796, 'grad_norm': 0.7821351289749146, 'learning_rate': 9.4e-05, 'epoch': 0.13}
{'loss': 0.6186, 'grad_norm': 3.041808843612671, 'learning_rate': 8.733333333333333e-05, 'epoch': 0.27}
{'loss': 0.6293, 'grad_norm': 1.4450652599334717, 'learning_rate': 8.066666666666667e-05, 'epoch': 0.4}
{'loss': 0.5665, 'grad_norm': 1.4925557374954224, 'learning_rate': 7.4e-05, 'epoch': 0.53}
{'loss': 0.5644, 'grad_norm': 1.503039836883545, 'learning_rate': 6.733333333333333e-05, 'epoch': 0.67}
{'eval_loss': 0.4775203466415405, 'eval_accuracy': 0.8033333333333333, 'eval_runtime': 3.7927, 'eval_samples_per_second': 79.1, 'eval_steps_per_second': 10.019, 'epoch': 0.67}
{'loss': 0.4269, 'grad_norm': 1.9867284297943115, 'learning_rate': 6.066666666666667e-05, 'epoch': 0.8}
{'loss': 0.5722, 'grad_norm': 3.0621354579925537, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.93}
{'loss': 0.3423, 'grad_norm': 2.3701376914978027, 'learning_rate': 4.7333333333333336e-05, 'epoch': 1.07}
{'loss': 0.3968, 'grad_norm': 9.607900619506836, 'learning_rate': 4.066666666666667e-05, 'epoch': 1.2}
{'loss': 0.2321, 'grad_norm': 3.9080681800842285, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.33}
{'eval_loss': 0.37428247928619385, 'eval_accuracy': 0.83, 'eval_runtime': 4.1834, 'eval_samples_per_second': 71.711, 'eval_steps_per_second': 9.083, 'epoch': 1.33}
{'loss': 0.2555, 'grad_norm': 7.629053115844727, 'learning_rate': 2.733333333333333e-05, 'epoch': 1.47}
{'loss': 0.2825, 'grad_norm': 5.45465612411499, 'learning_rate': 2.0666666666666666e-05, 'epoch': 1.6}
{'loss': 0.2116, 'grad_norm': 1.3811246156692505, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.73}
{'loss': 0.2311, 'grad_norm': 4.129777431488037, 'learning_rate': 7.333333333333334e-06, 'epoch': 1.87}
{'loss': 0.2605, 'grad_norm': 2.780629873275757, 'learning_rate': 6.666666666666667e-07, 'epoch': 2.0}
{'eval_loss': 0.3142576515674591, 'eval_accuracy': 0.87, 'eval_runtime': 3.7308, 'eval_samples_per_second': 80.412, 'eval_steps_per_second': 10.186, 'epoch': 2.0}
{'train_runtime': 103.6873, 'train_samples_per_second': 23.147, 'train_steps_per_second': 1.447, 'train_loss': 0.41798105239868166, 'epoch': 2.0}
***** train metrics *****
  epoch                    =         2.0
  total_flos               = 173208094GF
  train_loss               =       0.418
  train_runtime            =  0:01:43.68
  train_samples_per_second =      23.147
  train_steps_per_second   =       1.447
Labels of Dataset: ['female', 'male']
Classification Metrics
Accuracy     : 0.870
Precision     : 0.822
Recall        : 0.912 (Sensitivity)
Specificity   : 0.834
F1 Score      : 0.865
AUC           : 0.951
EER:          : 0.123
